#!/usr/bin/env python3
"""
DupeDeleter.py

Reads the 'pbtech_deals_playwright.csv' file generated by the scraper,
processes duplicates with specific logic, and saves the cleaned
data to 'filtered_catalog.csv'.

DEDUPLICATION LOGIC:
- Identifies duplicates by "Part Number" (or "Link" + "Name" as a fallback).
- If a "good" item (PromoCode is NOT on the undesirable list) and
  a "bad" item (PromoCode IS on the list) are found for the same product,
  this script will *always keep* the "good" item.
- If both items are "good" or both are "bad", it keeps the first one it finds.
"""

import csv
import os

# --- CONFIGURATION ---

# The file to read from (generated by your scraper)
INPUT_FILE = "pbtech_deals_playwright.csv"

# The new, clean file to create
OUTPUT_FILE = "filtered_catalog.csv"

# List of "undesirable" promo codes.
# These will be replaced by duplicates that DON'T have them.
UNDESIRABLE_PROMOS = [
    "FREE SHIPPING",
    "1 PER CUSTOMER",
    "NEW ARRIVAL",
    "REMANUFACTURED"
]

# --- END CONFIGURATION ---


def is_undesirable(promo_code):
    """Helper function to check if a promo code is on the bad list."""
    if not promo_code:
        return False
    
    # Check if any of the "undesirable" strings are *in* the promo code
    promo_upper = str(promo_code).upper()
    for code in UNDESIRABLE_PROMOS:
        if code in promo_upper:
            return True
    return False

def get_product_key(row):
    """Gets the unique identifier (Part Num or fallback) for a row."""
    part_num = row.get("Part Number")
    
    # Use Part Number if it exists and is not blank
    if part_num and str(part_num).strip():
        return part_num
    
    # Fallback to Link + Name if Part Number is missing
    link = row.get("Link") or ""
    name = row.get("Product name") or ""
    return f"{link}|{name}"


def main():
    """Main processing function."""
    
    # 1. Check if the input file exists
    if not os.path.exists(INPUT_FILE):
        print(f"Error: Input file '{INPUT_FILE}' not found.")
        print("Please run your 'MoreSiteScraper.py' script first to generate it.")
        return

    print(f"Reading items from '{INPUT_FILE}'...")
    all_rows = []
    fieldnames = []
    
    # 2. Read all data from the input CSV
    try:
        with open(INPUT_FILE, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            if not fieldnames:
                print("Error: CSV file appears to be empty or has no headers.")
                return
            
            all_rows = list(reader)
            
    except Exception as e:
        print(f"Error reading {INPUT_FILE}: {e}")
        return

    if not all_rows:
        print("Input file is empty. No processing needed.")
        return

    print(f"Processing {len(all_rows)} total items for duplicates...")

    # 3. Process duplicates
    # This dictionary will hold the *best* item found so far for each key
    best_items = {} 

    for row in all_rows:
        key = get_product_key(row)
        
        if key not in best_items:
            # This is the first time we've seen this product.
            # Add it as the "best" one for now.
            best_items[key] = row
        else:
            # Duplicate found. Compare it with the one we already stored.
            existing_item = best_items[key]
            
            existing_is_bad = is_undesirable(existing_item.get("PromoCode"))
            new_is_bad = is_undesirable(row.get("PromoCode"))

            # PREFERENCE LOGIC:
            # If the item we *already have* is "bad" (e.g., "FREE SHIPPING")
            # and this *new item* is "good" (e.g., "BFDEALS"),
            # then replace the bad one with this good one.
            if existing_is_bad and not new_is_bad:
                best_items[key] = row
            
            # In all other cases, we do nothing:
            # - If existing is "good" and new is "bad": Keep "good" existing.
            # - If both are "good": Keep the first one (existing).
            # - If both are "bad": Keep the first one (existing).
            
    # The final list is just the values from our dictionary
    final_list = list(best_items.values())

    print(f"Removed {len(all_rows) - len(final_list)} undesirable duplicates.")

    # 4. Write the clean data to the output file
    try:
        with open(OUTPUT_FILE, mode='w', encoding='utf-8', newline='') as f:
            # Use the same column headers from the original file
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(final_list)
            
        print(f"\nSuccessfully saved {len(final_list)} unique products to '{OUTPUT_FILE}'")
        
    except Exception as e:
        print(f"Error writing to {OUTPUT_FILE}: {e}")

if __name__ == "__main__":
    main()